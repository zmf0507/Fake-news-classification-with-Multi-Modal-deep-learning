{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "colab_type": "code",
    "id": "kImURHg5YYxz",
    "outputId": "5fc95183-45e8-4807-8335-bc69ace4d527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-07 18:22:37--  https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16775 (16K) [text/plain]\n",
      "Saving to: ‘tokenization.py’\n",
      "\n",
      "\r",
      "tokenization.py       0%[                    ]       0  --.-KB/s               \r",
      "tokenization.py     100%[===================>]  16.38K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2020-06-07 18:22:37 (1.60 MB/s) - ‘tokenization.py’ saved [16775/16775]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## bert tokenization library file\n",
    "!wget  https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "kVqn_Ch7GntE",
    "outputId": "4186c296-9c8b-447b-b4fa-942293768577"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 3.5MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.91\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cwOl_XsTYYyG"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling2D, Concatenate, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from keras.utils import np_utils\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import tokenization\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 702
    },
    "colab_type": "code",
    "id": "NHdjx4mvYYyV",
    "outputId": "a2e7dc01-7760-4434-a1d4-e53022cc4cd3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>imageId(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¿Se acuerdan de la película “El día después de...</td>\n",
       "      <td>21226711</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 223401 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>@milenagimon Miren a Sandy en NY!  Tremenda im...</td>\n",
       "      <td>192378571</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>Mon Oct 29 191123 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Huracán Sandy, me recuerda a...</td>\n",
       "      <td>132303095</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 181108 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY httpt.coe4JLBUfH</td>\n",
       "      <td>241995902</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 191533 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sa...</td>\n",
       "      <td>250315890</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 204602 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14622</th>\n",
       "      <td>443231991593304064</td>\n",
       "      <td>@BobombDom slaps TweetDeck with the PigFish ht...</td>\n",
       "      <td>2179310905</td>\n",
       "      <td>Da_Vault_Hunter</td>\n",
       "      <td>Tue Mar 11 03 48 36 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>pigFish_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14623</th>\n",
       "      <td>443086239127076865</td>\n",
       "      <td>New Species of Fish found in Brazil or just Re...</td>\n",
       "      <td>254843101</td>\n",
       "      <td>DjSituation_RC</td>\n",
       "      <td>Mon Mar 10 18 09 26 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>pigFish_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14624</th>\n",
       "      <td>442978105238753280</td>\n",
       "      <td>What do we call this #pigFISH http t.co4Bml62OD15</td>\n",
       "      <td>2367553228</td>\n",
       "      <td>Vivo1Vuyo</td>\n",
       "      <td>Mon Mar 10 10 59 45 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>pigFish_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14625</th>\n",
       "      <td>442753479782989824</td>\n",
       "      <td>Pigfish  E dopo il pescecane c'è il pesce maia...</td>\n",
       "      <td>603120231</td>\n",
       "      <td>CosimoTarta</td>\n",
       "      <td>Sun Mar 09 20 07 10 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>pigFish_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14626</th>\n",
       "      <td>442700377860104192</td>\n",
       "      <td>For those who can't decide between fish or mea...</td>\n",
       "      <td>25086784</td>\n",
       "      <td>johnszim</td>\n",
       "      <td>Sun Mar 09 16 36 09 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>pigFish_01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweetId  ...      imageId(s)\n",
       "0      263046056240115712  ...  sandyA_fake_46\n",
       "1      262995061304852481  ...  sandyA_fake_09\n",
       "2      262979898002534400  ...  sandyA_fake_09\n",
       "3      262996108400271360  ...  sandyA_fake_29\n",
       "4      263018881839411200  ...  sandyA_fake_15\n",
       "...                   ...  ...             ...\n",
       "14622  443231991593304064  ...      pigFish_01\n",
       "14623  443086239127076865  ...      pigFish_01\n",
       "14624  442978105238753280  ...      pigFish_01\n",
       "14625  442753479782989824  ...      pigFish_01\n",
       "14626  442700377860104192  ...      pigFish_01\n",
       "\n",
       "[14500 rows x 7 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reading training tweets data\n",
    "trainData = pd.read_csv(\"thesis-task/dataset/tweets.txt\", delimiter=\"\t\")\n",
    "splitData = pd.concat([pd.Series(row[\"tweetId\"],  row[\"imageId(s)\"].split(','))              \n",
    "                    for _, row in trainData.iterrows()]).reset_index(name=\"tweetId\")\n",
    "trainData = trainData.merge(splitData,how='outer',left_on=['tweetId'],right_on=['tweetId'])\n",
    "del trainData[\"imageId(s)\"]\n",
    "trainData.rename(columns={'index':'imageId(s)'}, inplace=True)\n",
    "trainData[\"imageId(s)\"] = trainData[\"imageId(s)\"].str.strip()\n",
    "for dir, _, files in os.walk('thesis-task/dataset/images/'):\n",
    "    imageList = files\n",
    "imgs = trainData[\"imageId(s)\"].unique()\n",
    "for i, image in enumerate(imageList):\n",
    "    image = image.split('.')[0]\n",
    "    imageList[i] = image\n",
    "\n",
    "imageList.remove('boston_fake_10')  ## format video;  not available\n",
    "trainData = trainData.loc[trainData['imageId(s)'].isin(imageList)]\n",
    "trainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "vdOmsS53YYyi",
    "outputId": "ff634c2d-7680-4a7c-a2a0-8b110385942d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fake</td>\n",
       "      <td>6844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>humor</td>\n",
       "      <td>2631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>real</td>\n",
       "      <td>5025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  Count\n",
       "0   fake   6844\n",
       "1  humor   2631\n",
       "2   real   5025"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## class wise train data distribution and their count \n",
    "dataDistribution = trainData.groupby(['label'], sort=True).size().reset_index(name='Count')\n",
    "dataDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "id": "upoqbI-NYYzX",
    "outputId": "c506e024-1c98-4c48-d62b-f423c6508a56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set class distribution\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fake</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>humor</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>real</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  Count\n",
       "0   fake    684\n",
       "1  humor    263\n",
       "2   real    502"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class distribution\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fake</td>\n",
       "      <td>6160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>humor</td>\n",
       "      <td>2368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>real</td>\n",
       "      <td>4519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  Count\n",
       "0   fake   6160\n",
       "1  humor   2368\n",
       "2   real   4519"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### generating validation dataset (train/validation split)\n",
    "valData = trainData.groupby(['label'],as_index=False).apply(lambda x: x.sample(frac=0.1))\n",
    "valData\n",
    "trainData = pd.concat([trainData,valData, valData]).drop_duplicates(keep=False)\n",
    "dataDistribution = valData.groupby(['label'], sort=True).size().reset_index(name='Count')\n",
    "print(\"Validation set class distribution\")\n",
    "display(dataDistribution.head())\n",
    "print(\"Training set class distribution\")\n",
    "dataDistribution = trainData.groupby(['label'], sort=True).size().reset_index(name='Count')\n",
    "dataDistribution.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "colab_type": "code",
    "id": "3mMk-dr1YYy_",
    "outputId": "fc22cc85-f0d8-4604-eae3-3ee11c4e489f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3865, 2)\n",
      "51\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>imageId(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578854927457349632</td>\n",
       "      <td>kereeen RT @Shyman33: Eclipse from ISS.... htt...</td>\n",
       "      <td>70824972</td>\n",
       "      <td>peay_s</td>\n",
       "      <td>Fri Mar 20 09:45:43 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>eclipse_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578874632670953472</td>\n",
       "      <td>Absolutely beautiful! RT @Shyman33: Eclipse fr...</td>\n",
       "      <td>344707006</td>\n",
       "      <td>JaredUcanChange</td>\n",
       "      <td>Fri Mar 20 11:04:02 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>eclipse_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578891261353984000</td>\n",
       "      <td>“@Shyman33: Eclipse from ISS.... http://t.co/C...</td>\n",
       "      <td>224839607</td>\n",
       "      <td>tpjp1231</td>\n",
       "      <td>Fri Mar 20 12:10:06 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>eclipse_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578846612312748032</td>\n",
       "      <td>Eclipse from ISS.... http://t.co/En87OtvsU6</td>\n",
       "      <td>134543073</td>\n",
       "      <td>Shyman33</td>\n",
       "      <td>Fri Mar 20 09:12:41 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>eclipse_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>578975333841551360</td>\n",
       "      <td>@ebonfigli: Éclipse vue de l'ISS... Autre chos...</td>\n",
       "      <td>1150728872</td>\n",
       "      <td>Epimethee_</td>\n",
       "      <td>Fri Mar 20 17:44:11 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>eclipse_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>583783882182430722</td>\n",
       "      <td>In honor of our fellow Students, we pray the g...</td>\n",
       "      <td>2492068440</td>\n",
       "      <td>Instaadict</td>\n",
       "      <td>Fri Apr 03 00:11:38 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>garissa_04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>583869095633911808</td>\n",
       "      <td>The horror image from inside #Kenya campus #Ga...</td>\n",
       "      <td>255109494</td>\n",
       "      <td>bleqdipSA</td>\n",
       "      <td>Fri Apr 03 05:50:14 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>garissa_04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>583813623425998848</td>\n",
       "      <td>#BREAKING \\n🔴WARNING! 🔞📷GRAPHIC CONTENT🔞\\nDeat...</td>\n",
       "      <td>514063845</td>\n",
       "      <td>marpipeas</td>\n",
       "      <td>Fri Apr 03 02:09:49 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>garissa_04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>583857897030975490</td>\n",
       "      <td>it hurts : @joansalihi : The image from\\ninsid...</td>\n",
       "      <td>1082284466</td>\n",
       "      <td>mage_amos</td>\n",
       "      <td>Fri Apr 03 05:05:44 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>garissa_04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>583871713370361856</td>\n",
       "      <td>The World must see this! @CNN @ABC  horror ima...</td>\n",
       "      <td>208488821</td>\n",
       "      <td>Tali_Munzhedzi</td>\n",
       "      <td>Fri Apr 03 06:00:38 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>garissa_04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2033 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweetId  ...  imageId(s)\n",
       "0     578854927457349632  ...  eclipse_01\n",
       "1     578874632670953472  ...  eclipse_01\n",
       "2     578891261353984000  ...  eclipse_01\n",
       "3     578846612312748032  ...  eclipse_01\n",
       "4     578975333841551360  ...  eclipse_01\n",
       "...                  ...  ...         ...\n",
       "2030  583783882182430722  ...  garissa_04\n",
       "2031  583869095633911808  ...  garissa_04\n",
       "2032  583813623425998848  ...  garissa_04\n",
       "2033  583857897030975490  ...  garissa_04\n",
       "2034  583871713370361856  ...  garissa_04\n",
       "\n",
       "[2033 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reading test tweets data\n",
    "testData = pd.read_csv(\"thesis-task/testset/tweets.txt\", delimiter=\"\t\")\n",
    "splitData = pd.concat([pd.Series(row[\"tweetId\"],  row[\"imageId(s)\"].split(','))              \n",
    "                    for _, row in testData.iterrows()]).reset_index(name=\"tweetId\")\n",
    "print(splitData.shape)\n",
    "testData = testData.merge(splitData,how='outer',left_on=['tweetId'],right_on=['tweetId'])\n",
    "del testData[\"imageId(s)\"]\n",
    "testData.rename(columns={'index':'imageId(s)'}, inplace=True)\n",
    "testData[\"imageId(s)\"] = testData[\"imageId(s)\"].str.strip()\n",
    "for dir, _, files in os.walk('thesis-task/testset/images/'):\n",
    "    imageList = files\n",
    "imgs = testData[\"imageId(s)\"].unique()\n",
    "print(len(imgs))\n",
    "for i, image in enumerate(imageList):\n",
    "    image = image.split('.')[0]\n",
    "    imageList[i] = image\n",
    "\n",
    "imageList.remove('syrian_boy_video')  ## format video;  not available\n",
    "testData = testData.loc[testData['imageId(s)'].isin(imageList)]\n",
    "testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "colab_type": "code",
    "id": "Ou-_msHpYYzI",
    "outputId": "f5ac092f-8204-434f-a79a-2e9515c996e7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fake</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>real</td>\n",
       "      <td>1212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  Count\n",
       "0  fake    821\n",
       "1  real   1212"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## class wise data distribution and their count \n",
    "dataDistribution = testData.groupby(['label'], sort=True).size().reset_index(name='Count')\n",
    "dataDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62bCv0r5YYzR"
   },
   "outputs": [],
   "source": [
    "## training labels encoding to one-hot vector\n",
    "trainLabels = trainData[\"label\"]\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(trainLabels)\n",
    "trainLabels = encoder.transform(trainLabels)\n",
    "trainLabels= np_utils.to_categorical(trainLabels)\n",
    "\n",
    "## test labels encoding to one-hot vector\n",
    "testLabels = testData[\"label\"]\n",
    "testLabels = encoder.transform(testLabels)\n",
    "testLabels= np_utils.to_categorical(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "G20hDMg5YYzn",
    "outputId": "9e644b66-83ad-4d04-bd88-da2a94b6cf43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "## loading bert pre-trained model\n",
    "%time\n",
    "bertUrl = \"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2\"\n",
    "bertLayer = hub.KerasLayer(bertUrl, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8bwPrp7YYzx"
   },
   "outputs": [],
   "source": [
    "## instantiating tokenizer class object\n",
    "vocab_file = bertLayer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bertLayer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dn2oSBqDYYz6",
    "outputId": "f8763ece-91b7-4156-bb1a-3dc7f4067147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Sandy wildin'... Sharks in the streets... No one is safe... #RNS #Hurricane #wave httpt.co2QKzhF6f \n",
      "\n",
      "\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "## randomly checking tweet and its tokens pre-processing\n",
    "tweet = data[\"tweetText\"].iloc[i]\n",
    "print(tweet, \"\\n\\n\")\n",
    "input_sequence = tokenizer.tokenize(tweet)\n",
    "print(len(input_sequence))\n",
    "tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "S8Fkna-KYY0J",
    "outputId": "996da62e-2b9e-4614-a577-2e5c206d3817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max Length of tweet for 95% of tweets : 50\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## finding the length distribution of tweets\n",
    "tweetsLength = [len(tokenizer.tokenize(tweet)) for tweet in trainData[\"tweetText\"].tolist()]\n",
    "%matplotlib inline\n",
    "np.random.seed(42)\n",
    "x = np.random.normal(size=1000)\n",
    "plt.hist(tweetsLength, density=False, bins=30) \n",
    "plt.xlabel('length of tweet')\n",
    "plt.ylabel('number of tweets')\n",
    "maxLen = int(np.percentile(tweetsLength, 95))\n",
    "print(\"max Length of tweet for 95% of tweets :\", maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0a35V2v5YY0V"
   },
   "outputs": [],
   "source": [
    "### encoding text sequence/tweet into bert model format\n",
    "def bert_encode(texts, tokenizer, max_len):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence =[\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XIi2bDdSYY0d"
   },
   "outputs": [],
   "source": [
    "### train/validation data generator for batch wise model training\n",
    "\n",
    "def dataGenerator(data, batch_size): \n",
    "    while True:\n",
    "        tweetsList = data[\"tweetText\"].tolist()\n",
    "        imageIds = data[\"imageId(s)\"].tolist()\n",
    "        trainInput = []\n",
    "        trainImages = []\n",
    "        labels = []\n",
    "        for dir, _, files in os.walk('thesis-task/dataset/images/'):\n",
    "            imageList = files\n",
    "        # print(len(imageList))\n",
    "        for i,imageId in enumerate(imageIds):\n",
    "            imageFile = [s for s in imageList if imageIds[i] in s][0]\n",
    "            image = cv2.imread('thesis-task/dataset/images/'+imageFile)\n",
    "            image = cv2.resize(image, (224,224))\n",
    "            trainImages.append(image)\n",
    "            labels.append(trainLabels[i])\n",
    "            if((i+1)%batch_size==0):\n",
    "                trainText = bert_encode(tweetsList[i-batch_size+1:i+1], tokenizer, int(maxLen))\n",
    "                trainInput = [trainText[0], trainText[1], trainText[2], np.array(trainImages)]\n",
    "                yield trainInput, np.array(labels)\n",
    "                trainInput = [] \n",
    "                trainImages = []\n",
    "                labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03ePSN_zYY0m"
   },
   "outputs": [],
   "source": [
    "### test data generator for batch wise model evaluation/testing\n",
    "def testDataGenerator(data, batch_size): \n",
    "    while True:\n",
    "      tweetsList = data[\"tweetText\"].tolist()\n",
    "      imageIds = data[\"imageId(s)\"].tolist()\n",
    "      trainInput = []\n",
    "      trainImages = []\n",
    "      labels = []\n",
    "      for dir, _, files in os.walk('thesis-task/testset/images/'):\n",
    "          imageList = files\n",
    "      # print(len(imageList))\n",
    "      for i,imageId in enumerate(imageIds):\n",
    "          imageFile = [s for s in imageList if imageIds[i] in s][0]\n",
    "          image = cv2.imread('thesis-task//testset/images/'+imageFile)\n",
    "          image = cv2.resize(image, (224,224))\n",
    "          trainImages.append(image)\n",
    "          labels.append(testLabels[i])\n",
    "          if((i+1)%batch_size==0):\n",
    "              trainText = bert_encode(tweetsList[i-batch_size+1:i+1], tokenizer, int(maxLen))\n",
    "              trainInput = [trainText[0], trainText[1], trainText[2], np.array(trainImages)]\n",
    "              yield trainInput, np.array(labels)\n",
    "              trainInput = [] \n",
    "              trainImages = []\n",
    "              labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRPXFo67YY0t"
   },
   "outputs": [],
   "source": [
    "## Bert and Vgg Model initialization\n",
    "def getBertModel(bert_layer, maxLen):\n",
    "    input_word_ids = Input(shape=(maxLen,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(maxLen,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(maxLen,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    print(clf_output)\n",
    "    hiddenLayer = Dense(768, activation='relu')(clf_output)  \n",
    "    dropOut = Dropout(0.4)(hiddenLayer)\n",
    "    hiddenLayer = Dense(32, activation='relu', name=\"bert-last\")(dropOut)\n",
    "    dropOut = Dropout(0.4)(hiddenLayer)\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=dropOut)\n",
    "#     model.compile(Adam(lr=1e-5), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def getVggModel():\n",
    "    vggModel = VGG19(weights='imagenet', include_top=False, input_shape = (224,224, 3))\n",
    "    vggModel.trainable = False\n",
    "    flatLayer = Flatten()\n",
    "    hiddenLayer1 = Dense(2742, activation='relu')\n",
    "    dropOut = Dropout(0.4)\n",
    "    hiddenLayer2 = Dense(32, activation='relu', name=\"vgg-last\")\n",
    "    dropOut = Dropout(0.4)\n",
    "    vggModel = tf.keras.Sequential([\n",
    "      vggModel,\n",
    "      flatLayer,\n",
    "      hiddenLayer1,\n",
    "      dropOut,\n",
    "      hiddenLayer2, dropOut\n",
    "    ])\n",
    "#     vggModel.compile(Adam(lr=1e-5), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    return vggModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EGc5MmofYY0-",
    "outputId": "b2a3320d-af91-40d0-aab0-c0d71cc9e024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 1s 0us/step\n",
      "Tensor(\"strided_slice:0\", shape=(None, 768), dtype=float32)\n",
      "Tensor(\"dropout_3/Identity:0\", shape=(None, 32), dtype=float32) Tensor(\"dropout_1_1/Identity:0\", shape=(None, 32), dtype=float32)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vgg19_input (InputLayer)        [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 177853441   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "vgg19 (Model)                   (None, 7, 7, 512)    20024384    vgg19_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 768)]        0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 25088)        0           vgg19[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 768)          590592      tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2742)         68794038    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 768)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             multiple             0           dense[0][0]                      \n",
      "                                                                 vgg-last[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bert-last (Dense)               (None, 32)           24608       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "vgg-last (Dense)                (None, 32)           87776       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           bert-last[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concat-layer (Concatenate)      (None, 64)           0           dropout_3[0][0]                  \n",
      "                                                                 dropout_1[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           4160        concat-layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 35)           2275        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 35)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            108         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 267,381,382\n",
      "Trainable params: 69,503,557\n",
      "Non-trainable params: 197,877,825\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## define model arhcitecture\n",
    "\n",
    "##define VGG model\n",
    "vggModel = getVggModel()\n",
    "\n",
    "## define BERT model\n",
    "bertModel = getBertModel(bertLayer, int(maxLen))\n",
    "\n",
    "## concatenate and combine models\n",
    "print(bertModel.output, vggModel.output)\n",
    "concatLayer = tf.keras.layers.Concatenate(name=\"concat-layer\")([bertModel.output, vggModel.output])\n",
    "combinedlayer = Dense(64, activation='relu')(concatLayer)\n",
    "combinedlayer = Dropout(0.4)(combinedlayer)\n",
    "combinedlayer = Dense(35, activation='relu')(combinedlayer)\n",
    "combinedlayer = Dropout(0.4)(combinedlayer)\n",
    "outputLayer = Dense(1, activation='sigmoid')(combinedlayer)\n",
    "\n",
    "model = Model(\n",
    "    [bertModel.input, vggModel.input],\n",
    "    outputLayer                        \n",
    ") \n",
    "model.summary()\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=Adam(lr=1e-3, decay=1e-3 / 200), metrics=[ 'accuracy', tf.keras.metrics.AUC(\n",
    "    num_thresholds=200, curve='ROC', summation_method='interpolation', name=None,\n",
    "    dtype=None, thresholds=None, multi_label=False, label_weights=None\n",
    ")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "TsWrKbNmYY1I",
    "outputId": "b32c7154-ec10-497e-8e24-1447ced30cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "160/160 [==============================] - 42s 263ms/step - loss: 0.0903 - accuracy: 0.9961 - auc: 0.9979 - val_loss: 1.1681e-06 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 42s 260ms/step - loss: 0.4139 - accuracy: 0.9500 - auc: 0.9817 - val_loss: 0.3282 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 41s 259ms/step - loss: 1.1778 - accuracy: 0.6148 - auc: 0.8528 - val_loss: 0.7619 - val_accuracy: 0.6458 - val_auc: 0.8732\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 42s 260ms/step - loss: 0.8330 - accuracy: 0.8781 - auc: 0.9534 - val_loss: 2.9313 - val_accuracy: 0.0000e+00 - val_auc: 0.5000\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 42s 260ms/step - loss: 0.5224 - accuracy: 0.6930 - auc: 0.9201 - val_loss: 1.1739e-06 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 42s 260ms/step - loss: 1.3051 - accuracy: 0.6703 - auc: 0.8669 - val_loss: 8.8952 - val_accuracy: 0.0000e+00 - val_auc: 0.2500\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 44s 277ms/step - loss: 0.0578 - accuracy: 0.9773 - auc: 0.9991 - val_loss: 30.0467 - val_accuracy: 0.0000e+00 - val_auc: 0.2500\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 77s 478ms/step - loss: 0.0235 - accuracy: 0.9930 - auc: 0.9993 - val_loss: 39.2383 - val_accuracy: 0.0000e+00 - val_auc: 0.2500\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 46s 285ms/step - loss: 1.3316 - accuracy: 0.8102 - auc: 0.9428 - val_loss: 0.9940 - val_accuracy: 0.0000e+00 - val_auc: 0.5000\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - ETA: 0s - loss: 1.0822 - accuracy: 0.5719 - auc: 0.7331WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 18 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 18 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "160/160 [==============================] - 40s 253ms/step - loss: 1.0822 - accuracy: 0.5719 - auc: 0.7331 - val_loss: 0.8073 - val_accuracy: 0.8500 - val_auc: 0.9065\n"
     ]
    }
   ],
   "source": [
    "### model training\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=8)\n",
    "trainGen = dataGenerator(trainData, batch_size=8)\n",
    "valGen = dataGenerator(valData, batch_size=8)\n",
    "history = model.fit_generator(\n",
    "    trainGen,\n",
    "    validation_data= valGen, validation_steps=18,\n",
    "    epochs=10, steps_per_epoch=160, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jeDrg_UzYY1V",
    "outputId": "a7c1f9ae-732c-46a4-9a27-ba62393c6337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "model_json = model.to_json()\n",
    "with open(\"spotfake-2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"spotfake-2.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "!cp -a spotfake-2.h5 \"/content/drive/My Drive/thesis-task/model-1\"\n",
    "!cp -a spotfake-2.json \"/content/drive/My Drive/thesis-task/model-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "colab_type": "code",
    "id": "dA9Bqx1EYY1d",
    "outputId": "f3fbeee0-28cf-4653-c918-36ea1d6300aa"
   },
   "outputs": [],
   "source": [
    "## getting prediction probabilities\n",
    "dataGen = testDataGenerator(testData, batch_size=8)\n",
    "yPred = model.predict_generator(dataGen, steps=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UO3zO2axPjCx",
    "outputId": "d3a12f8f-27ec-4c7f-9952-c7b16dc82bd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.408\n"
     ]
    }
   ],
   "source": [
    "def getBinaryAccuracy(yPred, yTrue):\n",
    "  acc = 0\n",
    "  count = 0\n",
    "  yPred = yPred > 0.5\n",
    "  for i in range(len(yPred)):\n",
    "    predClass = yPred[i]\n",
    "    trueClass = yTrue[i]\n",
    "    if(predClass==0):\n",
    "      count+=1\n",
    "    if(predClass==trueClass):\n",
    "      acc+=1\n",
    "  # print(count)\n",
    "  return acc/len(yPred)\n",
    "\n",
    "print(getAccuracy(yPred, testLabels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UcF1fC5RYY1n"
   },
   "source": [
    "# References\n",
    "1. https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2   \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "spotfake-implementation-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
